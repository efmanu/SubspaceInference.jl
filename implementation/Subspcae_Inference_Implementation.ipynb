{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subspace Inference for Bayesian Neural Network Using Julia\n",
    "\n",
    "### Introduction to uncertainty analysis in Deep Neural Network (DNN)\n",
    "\n",
    "Deep learning has led to a revolution in artificial intelligence, that has artificial neural networks capable of tackling more and more complex and challenging problems. The learning in this s networks can be supervised or unsupervised. However, there is a chance of overfitting in deep learning models. The Bayesian neural networks use Bayesian techniques to extract these uncertainties.\n",
    "\n",
    "### Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.sciweavers.org/tex2img.php?eq=P%28A%7CB%29%20%3D%20%5Cfrac%7BP%28A%29%2AP%28B%7CA%29%7D%7BP%28B%29%7D&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0\" align=\"center\" border=\"0\" alt=\"P(A|B) = \\frac{P(A)*P(B|A)}{P(B)}\" width=\"190\" height=\"46\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes theorem depends on the prior probability distributions and the likelihood value s to calculate the posterior probability. The posterior probability P(A|B) is proportional to prior and the likelihood probability distribution in the above equation. P(A|B) and P(B|A) is also called a conditional probability, and P(A) and P(B) is called marginal probability. P(B) calculation is a tedious process; therefore, approximate Bayesian inference is introduced. In approximate Bayesian inference, the posterior probability will be proportional to the product of prior probability distribution and the likelihood distribution. Similarly, the prior probability will be proportional to joint probability. The MCMC methods are used to sample from the posterior distribution using a joint probability distribution.\n",
    "\n",
    "### Bayesian Neural Network(BNN)\n",
    "\n",
    "Simply a BNN is a stochastic neural network trained using Bayesian inference methods.In BNN, the parameters are defined as a distribution instead of a single value as in the figure below:\n",
    "<img src=\"https://github.com/efmanu/SubspaceInference.jl/blob/master/implementation/bnn.png?raw=true\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of parameters represents the uncertainty in the neural network. This distribution corresponds to uncertainty is generated by using Bayes theorem with the help of MCMC methods. This MCM method generates samples of weight parameters from the posterior distribution of weights. The standard MCMC sampling methods are Metropolis-Hastings, Hamiltonian Monte Carlo and No-U-turn Sampler.\n",
    "\n",
    "### Why Subspace Inference:\n",
    "\n",
    "The conventional MCMC methods are suitable to generate uncertainties in NN. However, for the case DNN, the parameter space's size is large and time to calculate uncertainty distribution is also high. Subspace inference focus generates low dimensional subspace from the DNN parameters and performs Bayesian inference on that subspace to generate uncertainties.\n",
    "\n",
    "### Subspace Inference Algorithm\n",
    "\n",
    "The subspace inference uses a pretrained DNN with data D and model M\n",
    "1. Generate low dimensional subspace\n",
    "2. Execute Bayesian inference within this subspace\n",
    "3. Transform posterior of lower dimensional subspace to original dimension\n",
    "\n",
    "### Algorithm for subspace construction\n",
    "\n",
    "The inputs are `W0`: pretrained weights, `lr`: learning rate, `T`: Number steps, `f`: frequency to update deviation matrix, `N`: Maximum number of columns indeviation matrix, `R`: Rank of PCA, `P`: the projection matrix of low dimensional subspace. The variable `Wswa` is the mean weight.\n",
    "\n",
    "```julia\n",
    "Intialize mean weights Wswa = W0\n",
    "for i in 1:T\n",
    " Wi = Wi-1 - lr*gradient(cost(Wi, data)) #update using SGD\n",
    "if modulus(i,f) == 0 do\n",
    " n = i/f\n",
    "Wswa = (n*Wswa + wi)/(n+1)\n",
    "if Number_of_columns(D) == N do\n",
    " remove_columns(D)\n",
    "end\n",
    "Appends_column(D,Wi-Wswa)\n",
    "end\n",
    "end\n",
    "U,S,Vt = SVD(D)\n",
    "return P = U*S, Wswa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where SVD is the singular value decomposition, which returns thre outputs, U, S and Vt\n",
    "\n",
    "### Algorithm for subspace inference\n",
    "\n",
    "1. Initialize prior and proposal distribution of lower dimensional subspace,z\n",
    "2. Generate Forwadrd NN model weights `W_cap = Wswa + P*z`\n",
    "3. Generate likelihood projection model using proposal and forward NN\n",
    "4. Sample posterior of subspace based on joint probability\n",
    "\n",
    "### Implementation of Subspace Inference in Julia\n",
    "The subspace inference package can be find [SubspaceInference.jl](https://github.com/efmanu/SubspaceInference.jl).\n",
    "\n",
    "The ***subspace construction*** is implemented using [Flux.jl](https://github.com/FluxML/Flux.jl) to do the SGD update.\n",
    "\n",
    "```julia\n",
    "for i in 1:T\n",
    "    for d in data\n",
    "        gs = gradient(ps) do\n",
    "            training_loss = cost(d…)\n",
    "            return training_loss\n",
    "        end \n",
    "        Flux.update!(opt, ps, gs)\n",
    "        W = Array{Float64}(undef,0)\n",
    "        [append!(W, reshape(ps.order.data[i],:,1)) for i in 1:ps.params.dict.count];\n",
    "        if mod(i,c) == 0\n",
    "            n = i/c\n",
    "            W_swa = (n.*W_swa + W)./(n+1)\n",
    "            if(length(A) >= M*all_len)\n",
    "                A = A[1:(end - all_len)]\n",
    "            end\n",
    "            W_dev = W - W_swa\n",
    "            append!(A, W_dev)\n",
    "        end \n",
    "    end\n",
    "    println(\"Traing loss: \", training_loss,\" Epoch: \", i)\n",
    " end\n",
    "col_a = Int(floor(length(A)/all_len))\n",
    "A = reshape(A, all_len, col_a)\n",
    "U,s,V = TSVD.tsvd(A,M)\n",
    "P = U*LinearAlgebra.Diagonal(s)\n",
    "return W_swa, P, re\n",
    "```\n",
    "\n",
    "The ***subspace inference*** is implemented like below:\n",
    "\n",
    "```julia\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
