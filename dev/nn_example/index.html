<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>NN Example · SubspaceInference.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">SubspaceInference.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../subspace/">Subspace Construction</a></li><li><a class="tocitem" href="../inference/">Inference</a></li><li class="is-active"><a class="tocitem" href>NN Example</a></li><li><a class="tocitem" href="../node_example/">Neural ODE Example</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>NN Example</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>NN Example</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/efmanu/SubspaceInference.jl/blob/master/docs/src/nn_example.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Subspace-Inference-Based-Uncertainty-Analysis-for-Deep-Neural-Networks"><a class="docs-heading-anchor" href="#Subspace-Inference-Based-Uncertainty-Analysis-for-Deep-Neural-Networks">Subspace Inference Based Uncertainty Analysis for Deep Neural Networks</a><a id="Subspace-Inference-Based-Uncertainty-Analysis-for-Deep-Neural-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Subspace-Inference-Based-Uncertainty-Analysis-for-Deep-Neural-Networks" title="Permalink"></a></h1><h3 id="Bayes&#39;-Rule"><a class="docs-heading-anchor" href="#Bayes&#39;-Rule">Bayes&#39; Rule</a><a id="Bayes&#39;-Rule-1"></a><a class="docs-heading-anchor-permalink" href="#Bayes&#39;-Rule" title="Permalink"></a></h3><p>Bayes rule is a mathematical formula used to calculate the conditional probability of an event based on prior knowledge about the conditions that is related to the event. Bayes&#39; rule is expressed as:</p><p><span>$P(A|B) = \frac{P(B|A)*P(A))}{P(B)}$</span></p><p>Where,</p><ul><li><code>P(A|B)</code>: Conditional probability of event <code>A</code> occurring given <code>B</code> and also known as posterior and it is the updated belief about an event.</li><li><code>P(B|A)</code>: Conditional probability of event <code>B</code> occurring given <code>A</code>, known as likelihood</li><li><code>P(A)</code>  : Probability of event A, known as prior and it is the belief about the event.</li><li><code>P(B)</code>  : Probability of event B, known as evidence</li></ul><p>The <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Wikipedia page</a> clearly describes Bayes&#39; rule with examples.</p><h3 id="Bayesian-Inference"><a class="docs-heading-anchor" href="#Bayesian-Inference">Bayesian Inference</a><a id="Bayesian-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Inference" title="Permalink"></a></h3><p>The Bayesian inference depends on Bayes&#39; rule to update belief regarding a probability of an event to occur based on available observations and evidences. According to the book <a href="https://books.google.com.au/books?hl=en&amp;lr=&amp;id=6H_WDwAAQBAJ&amp;oi=fnd&amp;pg=PT8&amp;dq=statistical+rethinking+mcelreath&amp;ots=WhkgJHxQWl&amp;sig=La0bfuicltYRLmx7MpmwOuA17cc&amp;redir_esc=y#v=onepage&amp;q=statistical%20rethinking%20mcelreath&amp;f=false">Statistical Rethinking</a>, Bayesian inference uses Bayes&#39; rule to quantify the uncertainty in model and parameters. <a href="https://jvanderw.une.edu.au/L11introMCMC.pdf">Markov Chain  Monte Carlo Techniques (MCMC)</a> and <a href="https://stanford.edu/~jgrimmer/VariationalFinal.pdf">variational inference (VI)</a> are used for Bayesian inference.</p><h3 id="Julia-packages-for-Bayesian-inference"><a class="docs-heading-anchor" href="#Julia-packages-for-Bayesian-inference">Julia packages for Bayesian inference</a><a id="Julia-packages-for-Bayesian-inference-1"></a><a class="docs-heading-anchor-permalink" href="#Julia-packages-for-Bayesian-inference" title="Permalink"></a></h3><p><a href="https://github.com/TuringLang/Turing.jl">Turing.jl</a>, <a href="https://github.com/TuringLang/AdvancedMH.jl">AdvancedMH.jl</a>, <a href="https://github.com/TuringLang/AdvancedHMC.jl">AdvancedHMC.jl</a>, <a href="https://github.com/cscherrer/Soss.jl">Soss.jl</a>, <a href="https://github.com/StanJulia/Stan.jl">Stan.jl</a> are common packages provides Bayesian inference facility. <a href="https://github.com/TuringLang/Turing.jl">Turing.jl</a> contains MH, HMC, NUTS etc based MCMC sampler and variational inference based samplers too. Howeverm <a href="https://github.com/TuringLang/AdvancedMH.jl">AdvancedMH.jl</a> or <a href="https://github.com/TuringLang/AdvancedHMC.jl">AdvancedHMC.jl</a> only allows Bayesian inference with MH and HMC respectively. However, these algorithms can take input from user defined probability density functions for Bayesian inference.</p><h3 id="Bayesian-Neural-Networks"><a class="docs-heading-anchor" href="#Bayesian-Neural-Networks">Bayesian Neural Networks</a><a id="Bayesian-Neural-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Neural-Networks" title="Permalink"></a></h3><p><img src="../assets/bnn.png" alt="BNN"/></p><p>The Deep Neural Networks (DNN)  belongs to broader category of ANN with more hidden layers to extract more features from the training data which is capable to tackle more and more complex and challenging problems. However, there will be some  uncertainty in DNN parameters and it can be generated by using Bayesian inference. This technique is called as <a href="https://arxiv.org/pdf/1505.05424.pdf">Bayesian Neural Networks(BNN)</a> and this method uses MCMC and VI methods for uncertainty generation. The drawback on BNN is the time to generate the inference, when  the number of parameters in the neural networks (NN )increases, the uncertainty generation of NN parameters using Bayesian concept become more expensive in terms of time.</p><h3 id="Subspace-Inference"><a class="docs-heading-anchor" href="#Subspace-Inference">Subspace Inference</a><a id="Subspace-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Subspace-Inference" title="Permalink"></a></h3><p><a href="https://arxiv.org/abs/1907.07504">Subspace inference</a> method introduced to reduce the inference time in BNN by constructing a smaller subspace of actual NN Parameter space. This subspace is generated from the  principle  components of the deviation matrix of weight updation during training.</p><h3 id="Subspace-Inference-Algorithm"><a class="docs-heading-anchor" href="#Subspace-Inference-Algorithm">Subspace Inference Algorithm</a><a id="Subspace-Inference-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Subspace-Inference-Algorithm" title="Permalink"></a></h3><p>The subspace inference uses a pretrained DNN and it is implemented using following steps</p><ol><li>Generate low dimensional subspace</li><li>Execute Bayesian inference within this subspace</li><li>Transform posterior of lower dimensional subspace to original dimension</li></ol><h4 id="Algorithm-for-subspace-construction"><a class="docs-heading-anchor" href="#Algorithm-for-subspace-construction">Algorithm for subspace construction</a><a id="Algorithm-for-subspace-construction-1"></a><a class="docs-heading-anchor-permalink" href="#Algorithm-for-subspace-construction" title="Permalink"></a></h4><p>The subspace of NN model parameters are constructed by following steps:</p><ol><li>Initialize mean parameters to pretrained parameter value, <span>$W_{swa} = W_0$</span></li><li>For every epoch <span>$i$</span> <ol><li>Update parameter using SGD</li><li>Update mean parameters as  <span>$Wswa = (n*W_{swa} + \frac{W_{i}}{n+1})$</span>, where <span>$n = i/f$</span>. <span>$f$</span> is the weight update frequency</li><li>Calculate parameter deviation, <span>$W_{d} = W_{i}-W_{swa}$</span></li></ol></li><li>Do principle component analysis</li><li>Generate projection matrix</li></ol><h4 id="Algorithm-for-subspace-inference"><a class="docs-heading-anchor" href="#Algorithm-for-subspace-inference">Algorithm for subspace inference</a><a id="Algorithm-for-subspace-inference-1"></a><a class="docs-heading-anchor-permalink" href="#Algorithm-for-subspace-inference" title="Permalink"></a></h4><ol><li>Define proposal distribution of subspace</li><li>Define prior distribution of subspace</li><li>Define likelihood function, here it is a neural network</li><li>Sample subspace values using MCMC or VI samplers.</li></ol><h3 id="SubspaceInference.jl-Julia-package"><a class="docs-heading-anchor" href="#SubspaceInference.jl-Julia-package">SubspaceInference.jl Julia package</a><a id="SubspaceInference.jl-Julia-package-1"></a><a class="docs-heading-anchor-permalink" href="#SubspaceInference.jl-Julia-package" title="Permalink"></a></h3><p>The subspace inference method for DNN and ordinary differential equations (ODEs) are implemented as a package named <a href="https://github.com/efmanu/SubspaceInference.jl">SubspaceInference.jl</a> in Julia.  The subspace inference implementation by <a href="https://arxiv.org/abs/1907.07504">Izamailov</a> consider the deviations of last <span>$M$</span> (rank of PCA) columns. We have modified this algorithm by considering all deviation matrix during subspace construction to get more information about the subspace. Moreover, in <a href="https://arxiv.org/abs/1907.07504">Izamailov</a>&#39;s work, the posterior of subspace is updated based on the prior distribution of subspace only. We modified to generate subspace samples based on weight distribution. The prior distribution is defined as function,  and it takes the  subspace as the input  and  throws NN parameters prior distribution as output which is defined in the below pseudo function:</p><pre><code class="language-julia">function prior(z)
    W = Wswa + Pz
    return Normal(W,1.0)
end</code></pre><p>The advanced MH and HMC algorithms help to take prior distribution as functions instead of distribution. The <a href="https://github.com/efmanu/SubspaceInference.jl">SubspaceInference.jl</a> can be installed in Julia as:</p><pre><code class="language-julia">using Pkg
Pkg.add(&quot;https://github.com/efmanu/SubspaceInference.jl&quot;)</code></pre><h3 id="Example-of-subspace-inference-with-SubspaceInference.jl"><a class="docs-heading-anchor" href="#Example-of-subspace-inference-with-SubspaceInference.jl">Example of subspace inference with SubspaceInference.jl</a><a id="Example-of-subspace-inference-with-SubspaceInference.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Example-of-subspace-inference-with-SubspaceInference.jl" title="Permalink"></a></h3><p>Implementation of subspace inference for a multilayer perceptron with two input and one output  and three hidden layers is discussed in this section by referring <a href="https://github.com/wjmaddox/drbayes">python implementation</a> using the dataset from there. The example implementation is started with using some packages.</p><pre><code class="language-julia">using NPZ,Plots
using Flux, Flux: Data.DataLoader, Flux: @epochs
using BSON: @save, @load
using Zygote, Statistics, SubspaceInference;</code></pre><p>The sample data is loaded from <code>.npy</code> file found in <a href="https://github.com/wjmaddox/drbayes">python implementation</a>.  This data contains two columns and each columns named as <span>$x$</span> and  <span>$y$</span> respectively. The <span>$x$</span> is converted to features using features function. The feature function returns a matrix <span>$f$</span> with two columns. One column will be the <span>$\frac{x}{2}$</span> and the other column will be <span>$(\frac{x}{2})^2$</span>. </p><pre><code class="language-julia">data_ld = npzread(&quot;data.npy&quot;);
x, y = (data_ld[:, 1]&#39;, data_ld[:, 2]&#39;);
function features(x)
    return vcat(x./2, (x./2).^2)
end
f = features(x);</code></pre><p>The input data <span>$f$</span> and the output <span>$y$</span> zipped as <code>50</code> batches and shuffled using DataLoader available with Flux. The <span>$y$</span> from dataset is plotted against <span>$x$</span> as in the below figure.</p><pre><code class="language-julia">data =  DataLoader(f,y, batchsize=50, shuffle=true);
#plot data
scatter(data_ld[:,1],data_ld[:,2],color=[&quot;red&quot;], title=&quot;Dataset&quot;, legend=true)
</code></pre><p><img src="../assets/data_set.png" alt="Dataset"/></p><p>A simple multilayer perceptron is created as using Dense layer for implementing subspace inference example. This DNN contains 2 inputs, 1 output and hidden layers of <span>$[200,50,50]$</span> size. All layers other than the output layer contains the <code>ReLu</code> activation function.</p><pre><code class="language-julia">m = Chain(
    Dense(2,200,Flux.relu), 
    Dense(200,50,Flux.relu),
    Dense(50,50,Flux.relu),
    Dense(50,50,Flux.relu),
    Dense(50,1),
);
</code></pre><p>The mean squared error between input and output data is used as the loss function.</p><pre><code class="language-julia">L(x, y) = Flux.Losses.mse(m(x), y)/2;</code></pre><p>The Stochastic Gradient Descent(SGD) optimizer with learning rate of  <code>0.01</code> and momentum of <code>0.95</code> is used for updating parameters during DNN training.</p><pre><code class="language-julia">opt = Momentum(0.01, 0.95);</code></pre><p>The intialized parameters of DNN is extracted as below:</p><pre><code class="language-julia">ps = Flux.params(m);</code></pre><p>The callback function prints the loss for every training batch.</p><pre><code class="language-julia">callback() = @show(L(X,Y)) </code></pre><p>The initialized DNN is trained for 3000 epochs using <code>Flux.train!</code> function.</p><pre><code class="language-julia">@epochs 3000 Flux.train!(L, ps, data_ld, opt, cb = () -&gt; callback())</code></pre><p>Also, the DNN is trained <code>5</code> different iterations to plot the SGD solutions. The trained network  is saved after the training using <code>BSON</code> package for future use.</p><pre><code class="language-julia">epochs = 3000
for j in 1:5
   m = Chain(
           Dense(2,200,Flux.relu),
           Dense(200,50,Flux.relu),
           Dense(50,50,Flux.relu),
           Dense(50,50,Flux.relu),
           Dense(50,1),
   )
   ps = Flux.params(m)
   @epochs 1 Flux.train!(L, ps, data, opt, cb = () -&gt; callback()) 
   @save &quot;model_weights_$(j).bson&quot; ps
end</code></pre><p>These SGD solutions are used as the standard to compare the uncertainties generated by using subspace inference and plotted as using the below code:</p><pre><code class="language-julia">z = collect(range(-10.0, 10.0,length = 100))
inp = features(z&#39;)
trajectories = Array{Float64}(undef,100,5)
for i in 1:5
  @load &quot;model_weights_$(i).bson&quot; ps
  Flux.loadparams!(m, ps)
  out = m(inp)
  trajectories[:, i] = out&#39;
end
all_trj = Dict()
all_trj[&quot;1&quot;] = trajectories
SubspaceInference.plot_predictive(data_ld, all_trj, z, title=[&quot;SGD Solutions&quot;])</code></pre><p>This code first generate a collection of data named <code>z</code> between <code>-10.0</code> to <code>10.0</code>.  Then features will be generated as mentioned in the beginning. After that every saved DNN parameters loaded to model using <code>Flux.loadparams!()</code> function. Using this updated model, output is predicted and saved to <code>trajectories</code> array. After the prediction,  <code>trajectories</code> required to be added to a dictionary variable named <code>all_trj</code> because the <code>plot_predictive()</code> function support only <code>Dict</code> type. The plotted the SGD solution are shown in below figure.</p><p><img src="../assets/sgd_sol.png" alt="SGD Solutions"/></p><p>One of the pretrained model is used for subspace inference based uncertainty analysis. <code>subspace_inference()</code> function from <a href="https://github.com/efmanu/SubspaceInference.jl">SubspaceInference.jl</a> package used for this analysis. The loss function is modified to accept model as input for subspace inference, because new model parameters will be generated during sampling for the inference. The loss function will be:</p><pre><code class="language-julia">L1(m, x, y) = Flux.Losses.mse(m(x), y)</code></pre><p>This example considers subspace size of <code>3</code> and the pretrained model is updated for <code>10</code> epochs for deviation matrix generation. A new column will be added to deviation matrix during every batch training by setting moment update frequency, <code>c</code> as 1. During inference <code>100</code> subspace samples generated by setting <code>itr</code> variable. The subspace inference is generated using <code>RWMH</code> sampling algorithm  with proposal standard deviation of 0.1 as below:</p><pre><code class="language-julia">M = 3
T = 10
c= 1
σ_z = 0.1
itr = 100
 #cost function
all_chain, lp, W_swa = subspace_inference(m, L1, data, opt,   σ_z = 0.1,  itr =itr, T=T, c=1, M=M, print_freq=T, alg =:rwmh);</code></pre><p>The output of <code>subspace_inference()</code> having three variables. <code>chn</code> contains the different DNN weight samples generated from subspace samples. <code>lp</code> is the log probability of each sampling. This helps for subspace inference diagnostics. If the values of <code>lp</code> vector is constant, this means that the proposal by the samplers would be rejected.  If this scenario occurs, we have to try with different subspace size and different proposal distributions. The <code>lp</code> values will look like:</p><p>The effect of weight uncertainty is plotted by predicting output using modified DNN model with weight samples. The model is restructured using <code>re</code> function generated as:</p><pre><code class="language-julia">θ, re = Flux.destructure(m);</code></pre><p>For every DNN parameter samples, model is restructured and predicted the output for plotting. To plot, new input <span>$z$</span> is generated between  <code>-10</code> to <code>10</code>. This value is converted to features named <code>inp</code> and it will be fed to restructured DNN model from every parameter samples for the prediction. The predicted output is stored to a array named <code>trajectories</code>. After iteration, it is assigned to a dictionary variable as mentioned above.</p><pre><code class="language-julia">z = collect(range(-10.0, 10.0,length = 100))
inp = features(z&#39;)
trajectories = Array{Float64}(undef,100,itr)
for i in 1:itr
  m1 = re(all_chain[i])
  out = m1(inp)
  trajectories[:, i] = out&#39;
end
all_trajectories = Dict()
all_trajectories[&quot;1&quot;] = trajectories;</code></pre><p>The effect of DNN parameter uncertainties in output prediction using subspace inference is plotted using following line.</p><pre><code class="language-julia">SubspaceInference.plot_predictive(data_ld, all_trajectories, title=[&quot;Plot&quot;], z)</code></pre><p>and plotted as in below figure.</p><p><img src="../assets/sub_inference.png" alt="Subspace Inference"/></p><p>The light blue shaded area represents the effect of DNN parameter uncertainty in output prediction. The mean of uncertainty prediction is plotted in blue color and the red dots corresponds to data points. It is clear from the above figure that the uncertainty is higher in non data areas.</p><h3 id="Effect-of-different-subspace-sizes-in-uncertainty-analysis"><a class="docs-heading-anchor" href="#Effect-of-different-subspace-sizes-in-uncertainty-analysis">Effect of different subspace sizes in uncertainty analysis</a><a id="Effect-of-different-subspace-sizes-in-uncertainty-analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Effect-of-different-subspace-sizes-in-uncertainty-analysis" title="Permalink"></a></h3><p>This experiment analysis the effect of subspace size in uncertainty analysis. We considered subspace size <code>M</code> as <code>3</code>, <code>5</code>, <code>10</code> and <code>20</code>. The uncertainty is generated using the following code:</p><pre><code class="language-julia">

M = [3, 5, 10, 20] #Rank of PCA or Maximum columns in deviation matrix
T = 5 #Steps
itr = 100
all_trajectories = Dict()
z = collect(range(-10.0, 10.0,length = 100))
inp = features(z&#39;)
for mi in 1:4
    i = 1;
    @load &quot;model_weights_$(i).bson&quot; ps;
    Flux.loadparams!(m, ps);
    all_chain, lp, W_swa = subspace_inference(m, L, data, opt,
  σ_z = 1.0,  itr =itr, T=T, c=1, M=M[mi], print_freq=T, alg =:rwmh);    
    
    trajectories = Array{Float64}(undef,100,itr)
    for i in 1:itr
        m1 = re(all_chain[i])
        out = m1(inp)
        trajectories[:, i] = out&#39;
    end
    all_trajectories[&quot;$(mi)&quot;] = trajectories;
end
title = [&quot;subspace Size: 3&quot;,&quot;subspace Size: 5&quot;,&quot;subspace Size: 10&quot;,&quot;subspace Size: 20&quot;]

SubspaceInference.plot_predictive(data_ld, all_trajectories, z, title=title)</code></pre><p>In this code <code>M</code> is considered as an array with subspace sizes. Above figure illustrates the effect DNN parameter uncertainty that generated with different subspace sizes:</p><p><img src="../assets/com_diff_sub.png" alt="Effect of different subspace sizes"/></p><p>Above figure depicts that the uncertainty range is increases with higher subspace sizes.</p><h3 id="Effect-of-comparison-of-different-proposal-deviations"><a class="docs-heading-anchor" href="#Effect-of-comparison-of-different-proposal-deviations">Effect of comparison of different proposal deviations</a><a id="Effect-of-comparison-of-different-proposal-deviations-1"></a><a class="docs-heading-anchor-permalink" href="#Effect-of-comparison-of-different-proposal-deviations" title="Permalink"></a></h3><p>This focuses on the uncertainty outcomes due to different proposal standard deviations, <code>σ_z</code>. This simulation considers proposal deviations of <code>0.1</code> and <code>1.0</code> and following code is used:</p><pre><code class="language-julia">M = 10 #Rank of PCA or Maximum columns in deviation matrix
T = 5 #Steps
itr = 100
all_trajectories = Dict()
z = collect(range(-10.0, 10.0,length = 100))
inp = features(z&#39;)
σ_z = [0.1, 1.0]
for mi in 1:2
    i = 1;
    @load &quot;model_weights_$(i).bson&quot; ps;
    Flux.loadparams!(m, ps);
    all_chain, lp, W_swa = subspace_inference(m, L, data, opt,
  σ_z = σ_z[mi],  itr =itr, T=T, c=1, M=M, print_freq=T, alg =:rwmh);    
    
    trajectories = Array{Float64}(undef,100,itr)
    for i in 1:itr
        m1 = re(all_chain[i])
        out = m1(inp)
        trajectories[:, i] = out&#39;
    end
    all_trajectories[&quot;$(mi)&quot;] = trajectories;
end
title = [&quot;Std: 0.1&quot;, &quot;Std: 1.0&quot;]

SubspaceInference.plot_predictive(data_ld, all_trajectories, z, title=title)</code></pre><p><img src="../assets/st_comp_sub.png" alt="Effect of different proposal standard deviations"/></p><p>The above figure shows that for <code>σ_z = 1.0</code> generates larger uncertainty and it is not fitting mean prediction(blue line) with actual data.</p><h3 id="Autoencoder-based-subspace-inference"><a class="docs-heading-anchor" href="#Autoencoder-based-subspace-inference">Autoencoder based subspace inference</a><a id="Autoencoder-based-subspace-inference-1"></a><a class="docs-heading-anchor-permalink" href="#Autoencoder-based-subspace-inference" title="Permalink"></a></h3><p><a href="https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798">Autoencoders</a> are a special type of aritifical neural networks where the input is the same as the output. In this work, the encoder part of auto encoder is used to generate </p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../inference/">« Inference</a><a class="docs-footer-nextpage" href="../node_example/">Neural ODE Example »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 16 February 2021 12:32">Tuesday 16 February 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
