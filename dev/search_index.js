var documenterSearchIndex = {"docs":
[{"location":"node_example/#Introduction","page":"Neural ODE Example","title":"Introduction","text":"","category":"section"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"Neural ODE’s are introduced to model a system by combining machine learning and ODE’s together. This method is trying to model ODE representation of a system by using machine learning method instead of ","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"y = ML(x) neural ODE's trying to model as y^ = ML(x)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"DiffEqFlux.jl package helps to implement Neural ODE's in Julia.","category":"page"},{"location":"node_example/#Example-of-Neural-ODE","page":"Neural ODE Example","title":"Example of Neural ODE","text":"","category":"section"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"This example is taken from DiffEqFlux.jl Blog and considers Lokta Voltera ODE's is used for the study and it is represented as:","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"x^prime = alpha x + beta x y y^prime = -delta y + gamma x y","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"This ODE is solved using DifferentialEquations.jl package and the result is as shown below:","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"using DifferentialEquations\n\n#ODE function\nfunction lotka_volterra(du,u,p,t)\n  x, y = u\n  α, β, δ, γ = p\n  du[1] = dx = α*x - β*x*y\n  du[2] = dy = -δ*y + γ*x*y\nend\n#intial condition\nu0 = [1.0,1.0]\n#time span\ntspan = (0.0,10.0)\np = [1.5,1.0,3.0,1.0]\nprob = ODEProblem(lotka_volterra,u0,tspan,p)\n#solving ODE\nsol = solve(prob)\nusing Plots\n#plot ODE solution\nplot(sol)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"(Image: Lokta - Volterra ODE solution)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"Sometimes we won’t have exact knowledge of complete structure of non linear system to model using ODE’s. This case we  use Neural ODE’s to model the non linear system and to solve simply like training of Neural Network.","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"Neural ODE is discussed in this example with spiral ODE using DiffEqFlux.jl as below:","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"using DiffEqFlux\nusing Flux\nusing Flux: Data.DataLoader, @epochs\nusing DifferentialEquations, Plots","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The inital conditions of spiral ODE is set as:","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"#intial condition\nu0 = Float32[2.; 0.]\ndatasize = 30\n#indipendent variable range\ntspan = (0.0f0,1.5f0)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"DifferentialEquations.jl package is used to solve the ODE equations and this solution is used as the training data for NeuralODE.","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"#ode function\nfunction trueODEfunc(du,u,p,t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n#time span\nt = range(tspan[1],tspan[2],length=datasize)\nprob = ODEProblem(trueODEfunc,u0,tspan)\n#spiral ODE solution\node_data = Array(solve(prob,Tsit5(),saveat=t))","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The neural network (NN) for neural ODE is defined with a cubical transformation function and two dense hidden layers. The NN has two inputs and two outputs and this NN is defined as:","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"dudt = Chain(x -> x.^3,\n             Dense(2,50,tanh),\n             Dense(50,2))","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The neural ODE is incorporated with NN using NeuralODE() function as below:","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"n_ode = NeuralODE(dudt,tspan,Tsit5(),saveat=t,reltol=1e-7,abstol=1e-9)\nps = Flux.params(n_ode)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The prediction of ODE solution with randomly initialized network parameter is as shown below:","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"pred = n_ode(u0) # Get the prediction using the correct initial condition\nscatter(t,ode_data[1,:],label=\"data\")\nscatter!(t,pred[1,:],label=\"prediction\")","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"Above figure  illustrates the solution of neural ODE without training. (Image: Neural ODE solution before training)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"Now we can start training the Neural ODE. The function to predict is defined as:","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"#to predict solution from neural ODE\nfunction predict_n_ode()\n  n_ode(u0)\nend","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The sum squared error is used as the loss function and it is written as:","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"loss_n_ode() = sum(abs2,ode_data .- predict_n_ode())","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The solution is repeated 1000 times to make training data as:","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"data = Iterators.repeated((), 1000)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The NN parameter optimization is implemented using ADAM optimizer.","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"opt = ADAM(0.1)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"A callback function is defined to print loss during every epoch as:","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"cb = function () #callback function to observe training\n  display(loss_n_ode())\n  # plot current prediction against data\n  cur_pred = predict_n_ode()\n  pl = scatter(t,ode_data[1,:],label=\"data\")\n  scatter!(pl,t,cur_pred[1,:],label=\"prediction\")\n  display(plot(pl))\nend\n# Display the ODE with the initial parameter values.\ncb()","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The neural ODE is trained using Flux.train!() function as:","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"Flux.train!(loss_n_ode, ps, data, opt, cb = cb)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The solution from trained ODE is plotted as:","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"pred = n_ode(u0) # Get the prediction using the correct initial condition\nscatter(t,ode_data[1,:],label=\"data\")\nscatter!(t,pred[1,:],label=\"prediction\")","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"It is evident from the above figure that the predicted solution comes closer to actual solution. Therefore, it is a clear indication of proper training.","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"(Image: Neural ODE solution after training)","category":"page"},{"location":"node_example/#Importance-of-subspace-inference-in-Neural-ODE?","page":"Neural ODE Example","title":"Importance of subspace inference in Neural ODE?","text":"","category":"section"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The training of neural ODE’s provides slightly various parameters at different iterations. So, there will be an uncertainty in the solution using different neural ODE’s. Bayesian inference methods help to identify the uncertainties of the neural ODE parameters by using Markov Chain Monte Carlo (MCMC) or variational Inference (VI) samples. This Bayesian inference methods will be expensive when the number of parameters in the Neural ODE increases. Subspace Inference method is introduced to reduce time to calculate the uncertainties in neural ODE’s or Neural networks. SubspaceInference.jl is a Julia package developed for uncertainty analysis for Neural Networks and Neural ODE’s. This package supports NUTS, RWMH and MALA algorithm based Bayesian inferences.","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The subspace inference analysis using SubspaceInference.jl is discussed with spiral ODE. This package can be installed as:","category":"page"},{"location":"node_example/#Install-SubspaceInference-package","page":"Neural ODE Example","title":"Install SubspaceInference package","text":"","category":"section"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"using Pkg\nPkg.add(\"https://github.com/efmanu/SubspaceInference.jl\")","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"Before defining the ODE, we have to use some packages for inference","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"using BSON: @save, @load;\nusing Zygote, SubspaceInference, DifferentialEquations;\nusing Flux, DiffEqFlux, PyPlot, Distributions;\nusing Flux: Data.DataLoader, @epochs;","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"We can define spiral ODE using DifferentialEquations.jl. The spiral ODE consists of two dependent variable and the solution of ODE is calculated for independent variable t values from 0.0 to 1.5 with datasize=30 data points.","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"#initial conditions and time span\nlen = 100\n\n#intial conditions\nu0 = Array{Float64}(undef,2,len)\nu0 .= [2.; 0.]\n#datasize in solution\ndatasize = 30\ntspan = (0.0,1.5)\n\n#ode function\nfunction trueODEfunc(du,u,p,t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\n#time points\nt = range(tspan[1],tspan[2],length=datasize)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The ODE is solved to generate the training data. In this example, ODE output variables and the solutions is  for 30 data points. This solution is converted to a vector and it is used to fill ode_data matrix with len=100 columns. This matrix data will added with a noise of Normal(0.0, 0.1) and used to train the neural ODE.","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"ode_data = Array{Float64}(undef, 2*datasize, len)\nfor i in 1:len\n\tprob = ODEProblem(trueODEfunc,u0[:,i],tspan)\n\tode_data[:,i] = reshape(Array(solve(prob,Tsit5(),saveat=t))', :, 1)\nend\node_data_bkp = ode_data\node_data += rand(Normal(0.0,0.1), 2*datasize,len);","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The following code is used to plot the ODE solution with noise.","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"(fig, f_axes) = PyPlot.subplots(ncols=1, nrows=1)\nfor i in 1:len\n\tf_axes.scatter(t,vec(ode_data[1:1:datasize,i]), c=\"red\", alpha=0.3, marker=\"*\", label =\"data with noise\")\nend\nf_axes.plot(t,vec(ode_data_bkp[1:1:datasize,1]), c=\"red\", marker=\".\", label = \"data\")\nfig.show();","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"Above figure illustrates  the different solutions for spiral ODE.","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"(Image: Solution of ODE with noise)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The subspace inference methods use pretrained neural ODE and it is set up and trained as below:","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"dudt = Chain(x -> x.^3, Dense(2,15,tanh),\n             Dense(15,2))\nn_ode = NeuralODE(dudt,tspan,Tsit5(),saveat=t,\n\treltol=1e-7,abstol=1e-9);\n\nps = Flux.params(n_ode);\n\nsqnorm(x) = sum(abs2, x)\nL1(x, y) = sum(abs2, n_ode(vec(x)) .- \n\treshape(y[:,1], :,2)')+sum(sqnorm, Flux.params(n_ode))/100\n#call back\ncb = function () #callback function to observe training\n  @show L1(u0[:,1], ode_data_bkp[:,1])\nend\n\n#optiizer\nopt = ADAM(0.1);\n\n#format data\nX = u0 #input\nY =ode_data #output \n\ndata =  DataLoader(X,Y);\n\n@epochs 4 Flux.train!(L1, ps, data, opt);\ncb();","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The solution of ODE with pretrained network is shown in the below figure:","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"(fig, f_axes) = PyPlot.subplots(ncols=1, nrows=1)\npred = n_ode(vec(u0[:,1])) # Get the prediction using the correct initial condition\nf_axes.plot(t,vec(ode_data_bkp[1:datasize,1]), c=\"red\", marker=\".\", label = \"data\")\nf_axes.plot(t,vec(pred[1,:]), c=\"green\", marker=\".\", label =\"prediction\")\nf_axes.legend()\nfig.show()","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"(Image: Predicted solution)","category":"page"},{"location":"node_example/#Subspace-Inference-for-Neural-ODE","page":"Neural ODE Example","title":"Subspace Inference for Neural ODE","text":"","category":"section"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"We have to modify the loss function for subspace construction because this algorithm updates weight parameters every time and calculate the loss.","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"L1(m, x, y) = sum(abs2, m(vec(x)) .- reshape(y[:,1], :,2)')+sum(sqnorm, Flux.params(m))/100;","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The subspace inference is generated for subspace size of 3 with 100 iterations as below. This algorithm generates uncertainties using MH algorithm with subspace with proposal distribution of 0.1. During inference, the posterior samples of subspace is generated by considering the prior distribution of neural network parameters.","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"T = 1\nM = 3\nitr = 100\nσ_z = 0.1 #proposal distribution\n\n#do subspace inference\nchn, lp, W_swa = SubspaceInference.subspace_inference(n_ode, L1, data, opt;\n\tσ_z = σ_z, itr =itr, T=T, M=M,  alg =:mh);\n\nns = length(chn)\n\ntrajectories = Array{Float64}(undef,2*datasize,ns)\nfor i in 1:ns\n  new_model = SubspaceInference.model_re(n_ode, chn[i])\n  out = new_model(u0[:,1])\n  reshape(Array(out)',:,1)\n  trajectories[:, i] = reshape(Array(out)',:,1)\nend\n\nall_trajectories = Dict()\nall_trajectories[1] = trajectories\ntitle = [\"Subspace Size: $M\"]\n\nSubspaceInference.plot_node(t, all_trajectories, ode_data_bkp, ode_data, 2, datasize, title)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The uncertainties in solution is plotted for two variables in the below two figures. The blue color shaded area is corresponds to generated uncertainty information and red shaded area corresponds to the noise in the trained data.","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"(Image: Uncertainty in var 1 solution) (Image: Uncertainty in var 2 solution)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The plot of variable 1 against variable 2 for subspace of 3 with 1.0 proposal distribution is Illustrated  in the below figure. ","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"(Image: Variable 1 solution vs variable 2)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"The below figure discuss the uncertainties in predictions as well as in forecasting.","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"(Image: Prediction and Forecasting)","category":"page"},{"location":"node_example/#Effect-of-different-subspace-sizes-in-neural-ODE-uncertainty-generation","page":"Neural ODE Example","title":"Effect of different subspace sizes in neural ODE uncertainty generation","text":"","category":"section"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"This experiment focuses on the uncertainty outcomes due to different proposal standard deviations, σ_z. This simulation considers proposal deviations of 0.1 and 1.0 and following code is used:","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"T = 1\nM = [3,5,10,15]\nitr = 100\nσ_z = 0.1\nalg = :hmc\nall_trajectories = Dict()\nfor ti in 1:4\n  @load \"n_ode_weights_30r.bson\" ps;\n  Flux.loadparams!(n_ode, ps);\n\n  #do subspace inference\n  chn, lp, W_swa = SubspaceInference.subspace_inference(n_ode, L1, data, opt;\n    σ_z = σ_z, itr =itr, T=T, M=M[ti],  alg =alg);\n\n  ns = length(chn)\n\n  trajectories = Array{Float64}(undef,2*datasize,ns)\n  for i in 1:ns\n    new_model = SubspaceInference.model_re(n_ode, chn[i])\n    out = new_model(u0[:,1])\n    reshape(Array(out)',:,1)\n    trajectories[:, i] = reshape(Array(out)',:,1)\n  end\n  \n  all_trajectories[ti] = trajectories\nend\ntitle = [\"Subspace Size:3\",\"Subspace Size:5\",\"Subspace Size:10\",\"Subspace Size:15\"]\n\nSubspaceInference.plot_node(t, all_trajectories, ode_data_bkp, ode_data, 2, datasize, title)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"(Image: Effect of different proposal standard deviations of variable 1) (Image: Effect of different proposal standard deviations of variable 2)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"It is evident from abve figures that the uncertainty information decreases with increase in subspace sizes. Moreover, the current parameters used for the experiment is not enough to cover the complete data uncertainty.","category":"page"},{"location":"node_example/#Effect-of-different-proposal-distributions-in-neural-ODE-uncertainty-generation","page":"Neural ODE Example","title":"Effect of different proposal distributions in neural ODE uncertainty generation","text":"","category":"section"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"T = 1\nM = 5\nitr = 100\nσ_z = [0.1, 1.0]\nalg = :hmc\nall_trajectories = Dict()\nfor ti in 1:4\n  @load \"n_ode_weights_30r.bson\" ps;\n  Flux.loadparams!(n_ode, ps);\n\n  #do subspace inference\n  chn, lp, W_swa = SubspaceInference.subspace_inference(n_ode, L1, data, opt;\n    σ_z = σ_z[ti], itr =itr, T=T, M=M,  alg =alg);\n\n  ns = length(chn)\n\n  trajectories = Array{Float64}(undef,2*datasize,ns)\n  for i in 1:ns\n    new_model = SubspaceInference.model_re(n_ode, chn[i])\n    out = new_model(u0[:,1])\n    reshape(Array(out)',:,1)\n    trajectories[:, i] = reshape(Array(out)',:,1)\n  end\n  \n  all_trajectories[ti] = trajectories\nend\ntitle = [\"Std: 0.1\",\"Std: 1.0\"]\n\nSubspaceInference.plot_node(t, all_trajectories, ode_data_bkp, ode_data, 2, datasize, title)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"(Image: Effect of different proposal standard deviations of variable 1) (Image: Effect of different proposal standard deviations of variable 2)","category":"page"},{"location":"node_example/","page":"Neural ODE Example","title":"Neural ODE Example","text":"It is noticeable from the above figures that the uncertainty information for proposal standard deviation 1.0 is too high compared to standard deviation of 0.1. ","category":"page"},{"location":"subspace/#SubspaceInference.jl","page":"Subspace Construction","title":"SubspaceInference.jl","text":"","category":"section"},{"location":"subspace/","page":"Subspace Construction","title":"Subspace Construction","text":"CurrentModule = SubspaceInference\nDocTestSetup = quote\n    using SubspaceInference\nend","category":"page"},{"location":"subspace/","page":"Subspace Construction","title":"Subspace Construction","text":"The subspace inference method for Deep Neural Networks (DNN) and ordinary differential equations (ODEs) are implemented as a package named SubspaceInference.jl in Julia.","category":"page"},{"location":"subspace/#Subspace-Construction","page":"Subspace Construction","title":"Subspace Construction","text":"","category":"section"},{"location":"subspace/#PCA-based-subspace-construction","page":"Subspace Construction","title":"PCA based subspace construction","text":"","category":"section"},{"location":"subspace/","page":"Subspace Construction","title":"Subspace Construction","text":"subspace_construction(model, cost, data, opt; T = 10, c = 1, M = 3, print_freq = 1)","category":"page"},{"location":"subspace/#SubspaceInference.subspace_construction-NTuple{4,Any}","page":"Subspace Construction","title":"SubspaceInference.subspace_construction","text":"subspace_construction(model, cost, data, opt; T = 10, c = 1, M = 3, print_freq = 1)\n\nIzmailov, P., Maddox, W. J., Kirichenko, P., Garipov, T., Vetrov, D., & Wilson, A. G. (2020, August). Subspace inference for Bayesian deep learning.  In Uncertainty in Artificial Intelligence (pp. 1169-1179). PMLR.\n\nTo construct subspace from pretrained weights.\n\nInput Arguments\n\nmodel \t : Machine learning model. Eg: Chain(Dense(10,2)). Model should be created with Chain in Flux\ncost  \t : Cost function. Eg: L(x, y) = Flux.Losses.mse(m(x), y)\ndata \t : Inputs and outputs. Eg:\tX = rand(10,100); Y = rand(2,100); data = DataLoader(X,Y);\nopt\t\t : Optimzer. Eg: opt = ADAM(0.1)\n\nKeyword Arguments\n\nT \t\t  : Number of steps for subspace calculation. Eg: T= 1\nc \t\t  : Moment update frequency. Eg: c = 1\nM \t\t  : Maximum number of columns in deviation matrix. Eg: M= 2\nprint_freq: Loss printing frequency\n\nOutputs\n\nW_swa    : Mean weights\nP \t\t : Projection Matrix\n\n\n\n\n\n","category":"method"},{"location":"subspace/#Diffusion-map-based-subspace-construction","page":"Subspace Construction","title":"Diffusion map based subspace construction","text":"","category":"section"},{"location":"subspace/","page":"Subspace Construction","title":"Subspace Construction","text":"diffusion_subspace(model, cost, data, opt; T = 10, c = 1, M = 3, print_freq = 1)","category":"page"},{"location":"subspace/#SubspaceInference.diffusion_subspace-NTuple{4,Any}","page":"Subspace Construction","title":"SubspaceInference.diffusion_subspace","text":"diffusion_subspace(model, cost, data, opt; T = 10, c = 1, M = 3, print_freq = 1)\n\nTo construct subspace from pretrained weights using diffusion maps.\n\nInput Arguments\n\nmodel \t : Machine learning model. Eg: Chain(Dense(10,2)). Model should be created with Chain in Flux\ncost  \t : Cost function. Eg: L(x, y) = Flux.Losses.mse(m(x), y)\ndata \t : Inputs and outputs. Eg:\tX = rand(10,100); Y = rand(2,100); data = DataLoader(X,Y);\nopt\t\t : Optimzer. Eg: opt = ADAM(0.1)\n\nKeyword Arguments\n\nT \t\t  : Number of steps for subspace calculation. Eg: T= 1\nc \t\t  : Moment update frequency. Eg: c = 1\nM \t\t  : Maximum number of columns in deviation matrix. Eg: M= 2\nprint_freq: Loss printing frequency\n\nOutputs\n\nW_swa    : Mean weights\nP \t\t : Projection Matrix\n\n\n\n\n\n","category":"method"},{"location":"subspace/#Autoencoder-based-subspace-construction","page":"Subspace Construction","title":"Autoencoder based subspace construction","text":"","category":"section"},{"location":"subspace/","page":"Subspace Construction","title":"Subspace Construction","text":"auto_encoder_subspace(model, cost, data, opt, encoder, decoder; T = 10, c = 1, M = 3, print_freq = 1)","category":"page"},{"location":"subspace/#SubspaceInference.auto_encoder_subspace-NTuple{6,Any}","page":"Subspace Construction","title":"SubspaceInference.auto_encoder_subspace","text":"auto_encoder_subspace(model, cost, data, opt, encoder, decoder; T = 10, c = 1, M = 3, print_freq = 1\n\n)\n\nTo construct subspace from pretrained weights using autoencoders.\n\nInput Arguments\n\nmodel \t : Machine learning model. Eg: Chain(Dense(10,2)). Model should be created with Chain in Flux\ncost  \t : Cost function. Eg: L(x, y) = Flux.Losses.mse(m(x), y)\ndata \t : Inputs and outputs. Eg:\tX = rand(10,100); Y = rand(2,100); data = DataLoader(X,Y);\nopt\t\t : Optimzer. Eg: opt = ADAM(0.1)\nencoder\t : Encoder to generate subspace from NN or Neural ODE parameters\ndecoder\t : Decoder to generate NN or Neural ODE parameters from subspace\n\nKeyword Arguments\n\nT \t\t  : Number of steps for subspace calculation. Eg: T= 1\nc \t\t  : Moment update frequency. Eg: c = 1\nM \t\t  : Maximum number of columns in deviation matrix. Eg: M= 2\nprint_freq: Loss printing frequency\n\nOutputs\n\nW_swa    : Mean weights\ndecoder  : Trained decoder to generate NN or Neural ODE parameters from subspace\n\n\n\n\n\n","category":"method"},{"location":"nn_example/#Subspace-Inference-Based-Uncertainty-Analysis-for-Deep-Neural-Networks","page":"NN Example","title":"Subspace Inference Based Uncertainty Analysis for Deep Neural Networks","text":"","category":"section"},{"location":"nn_example/#Bayes'-Rule","page":"NN Example","title":"Bayes' Rule","text":"","category":"section"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"Bayes rule is a mathematical formula used to calculate the conditional probability of an event based on prior knowledge about the conditions that is related to the event. Bayes' rule is expressed as:","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"P(AB) = fracP(BA)*P(A))P(B)","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"Where,","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"P(A|B): Conditional probability of event A occurring given B and also known as posterior and it is the updated belief about an event.\nP(B|A): Conditional probability of event B occurring given A, known as likelihood\nP(A)  : Probability of event A, known as prior and it is the belief about the event.\nP(B)  : Probability of event B, known as evidence","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The Wikipedia page clearly describes Bayes' rule with examples.","category":"page"},{"location":"nn_example/#Bayesian-Inference","page":"NN Example","title":"Bayesian Inference","text":"","category":"section"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The Bayesian inference depends on Bayes' rule to update belief regarding a probability of an event to occur based on available observations and evidences. According to the book Statistical Rethinking, Bayesian inference uses Bayes' rule to quantify the uncertainty in model and parameters. Markov Chain  Monte Carlo Techniques (MCMC) and variational inference (VI) are used for Bayesian inference.","category":"page"},{"location":"nn_example/#Julia-packages-for-Bayesian-inference","page":"NN Example","title":"Julia packages for Bayesian inference","text":"","category":"section"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"Turing.jl, AdvancedMH.jl, AdvancedHMC.jl, Soss.jl, Stan.jl are common packages provides Bayesian inference facility. Turing.jl contains MH, HMC, NUTS etc based MCMC sampler and variational inference based samplers too. Howeverm AdvancedMH.jl or AdvancedHMC.jl only allows Bayesian inference with MH and HMC respectively. However, these algorithms can take input from user defined probability density functions for Bayesian inference.","category":"page"},{"location":"nn_example/#Bayesian-Neural-Networks","page":"NN Example","title":"Bayesian Neural Networks","text":"","category":"section"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"(Image: BNN)","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The Deep Neural Networks (DNN)  belongs to broader category of ANN with more hidden layers to extract more features from the training data which is capable to tackle more and more complex and challenging problems. However, there will be some  uncertainty in DNN parameters and it can be generated by using Bayesian inference. This technique is called as Bayesian Neural Networks(BNN) and this method uses MCMC and VI methods for uncertainty generation. The drawback on BNN is the time to generate the inference, when  the number of parameters in the neural networks (NN )increases, the uncertainty generation of NN parameters using Bayesian concept become more expensive in terms of time.","category":"page"},{"location":"nn_example/#Subspace-Inference","page":"NN Example","title":"Subspace Inference","text":"","category":"section"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"Subspace inference method introduced to reduce the inference time in BNN by constructing a smaller subspace of actual NN Parameter space. This subspace is generated from the  principle  components of the deviation matrix of weight updation during training.","category":"page"},{"location":"nn_example/#Subspace-Inference-Algorithm","page":"NN Example","title":"Subspace Inference Algorithm","text":"","category":"section"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The subspace inference uses a pretrained DNN and it is implemented using following steps","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"Generate low dimensional subspace\nExecute Bayesian inference within this subspace\nTransform posterior of lower dimensional subspace to original dimension","category":"page"},{"location":"nn_example/#Algorithm-for-subspace-construction","page":"NN Example","title":"Algorithm for subspace construction","text":"","category":"section"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The subspace of NN model parameters are constructed by following steps:","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"Initialize mean parameters to pretrained parameter value, W_swa = W_0\nFor every epoch i   2.1 Update parameter using SGD  2.2 Update mean parameters as  Wswa = (n*W_swa + fracW_in+1) where n = if. f is the weight update frequency  2.3 Calculate parameter deviation, W_d = W_i-W_swa\nDo principle component analysis\nGenerate projection matrix","category":"page"},{"location":"nn_example/#Algorithm-for-subspace-inference","page":"NN Example","title":"Algorithm for subspace inference","text":"","category":"section"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"Define proposal distribution of subspace\nDefine prior distribution of subspace\nDefine likelihood function, here it is a neural network\nSample subspace values using MCMC or VI samplers.","category":"page"},{"location":"nn_example/#SubspaceInference.jl-Julia-package","page":"NN Example","title":"SubspaceInference.jl Julia package","text":"","category":"section"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The subspace inference method for DNN and ordinary differential equations (ODEs) are implemented as a package named SubspaceInference.jl in Julia.  The subspace inference implementation by Izamailov consider the deviations of last M (rank of PCA) columns. We have modified this algorithm by considering all deviation matrix during subspace construction to get more information about the subspace. Moreover, in Izamailov's work, the posterior of subspace is updated based on the prior distribution of subspace only. We modified to generate subspace samples based on weight distribution. The prior distribution is defined as function,  and it takes the  subspace as the input  and  throws NN parameters prior distribution as output which is defined in the below pseudo function:","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"function prior(z)\n    W = Wswa + Pz\n    return Normal(W,1.0)\nend","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The advanced MH and HMC algorithms help to take prior distribution as functions instead of distribution. The SubspaceInference.jl can be installed in Julia as:","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"using Pkg\nPkg.add(\"https://github.com/efmanu/SubspaceInference.jl\")","category":"page"},{"location":"nn_example/#Example-of-subspace-inference-with-SubspaceInference.jl","page":"NN Example","title":"Example of subspace inference with SubspaceInference.jl","text":"","category":"section"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"Implementation of subspace inference for a multilayer perceptron with two input and one output  and three hidden layers is discussed in this section by referring python implementation using the dataset from there. The example implementation is started with using some packages.","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"using NPZ,Plots\nusing Flux, Flux: Data.DataLoader, Flux: @epochs\nusing BSON: @save, @load\nusing Zygote, Statistics, SubspaceInference;","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The sample data is loaded from .npy file found in python implementation.  This data contains two columns and each columns named as x and  y respectively. The x is converted to features using features function. The feature function returns a matrix f with two columns. One column will be the x2 and the other column will be (fracx2)^2. ","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"data_ld = npzread(\"data.npy\");\nx, y = (data_ld[:, 1]', data_ld[:, 2]');\nfunction features(x)\n    return vcat(x./2, (x./2).^2)\nend\nf = features(x);","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The input data f and the output y zipped as 50 batches and shuffled using DataLoader available with Flux. The y from dataset is plotted against x as in the below figure.","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"data =  DataLoader(f,y, batchsize=50, shuffle=true);\n#plot data\nscatter(data_ld[:,1],data_ld[:,2],color=[\"red\"], title=\"Dataset\", legend=true)\n","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"(Image: Dataset)","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"A simple multilayer perceptron is created as using Dense layer for implementing subspace inference example. This DNN contains 2 inputs, 1 output and hidden layers of 2005050 size. All layers other than the output layer contains the ReLu activation function.","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"m = Chain(\n    Dense(2,200,Flux.relu), \n    Dense(200,50,Flux.relu),\n    Dense(50,50,Flux.relu),\n    Dense(50,50,Flux.relu),\n    Dense(50,1),\n);\n","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The mean squared error between input and output data is used as the loss function.","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"L(x, y) = Flux.Losses.mse(m(x), y)/2;","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The Stochastic Gradient Descent(SGD) optimizer with learning rate of  0.01 and momentum of 0.95 is used for updating parameters during DNN training.","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"opt = Momentum(0.01, 0.95);","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The intialized parameters of DNN is extracted as below:","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"ps = Flux.params(m);","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The callback function prints the loss for every training batch.","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"callback() = @show(L(X,Y)) ","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The initialized DNN is trained for 3000 epochs using Flux.train! function.","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"@epochs 3000 Flux.train!(L, ps, data_ld, opt, cb = () -> callback())","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"Also, the DNN is trained 5 different iterations to plot the SGD solutions. The trained network  is saved after the training using BSON package for future use.","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"epochs = 3000\nfor j in 1:5\n   m = Chain(\n           Dense(2,200,Flux.relu),\n           Dense(200,50,Flux.relu),\n           Dense(50,50,Flux.relu),\n           Dense(50,50,Flux.relu),\n           Dense(50,1),\n   )\n   ps = Flux.params(m)\n   @epochs 1 Flux.train!(L, ps, data, opt, cb = () -> callback()) \n   @save \"model_weights_$(j).bson\" ps\nend","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"These SGD solutions are used as the standard to compare the uncertainties generated by using subspace inference and plotted as using the below code:","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"z = collect(range(-10.0, 10.0,length = 100))\ninp = features(z')\ntrajectories = Array{Float64}(undef,100,5)\nfor i in 1:5\n  @load \"model_weights_$(i).bson\" ps\n  Flux.loadparams!(m, ps)\n  out = m(inp)\n  trajectories[:, i] = out'\nend\nall_trj = Dict()\nall_trj[\"1\"] = trajectories\nSubspaceInference.plot_predictive(data_ld, all_trj, z, title=[\"SGD Solutions\"])","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"This code first generate a collection of data named z between -10.0 to 10.0.  Then features will be generated as mentioned in the beginning. After that every saved DNN parameters loaded to model using Flux.loadparams!() function. Using this updated model, output is predicted and saved to trajectories array. After the prediction,  trajectories required to be added to a dictionary variable named all_trj because the plot_predictive() function support only Dict type. The plotted the SGD solution are shown in below figure.","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"(Image: SGD Solutions)","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"One of the pretrained model is used for subspace inference based uncertainty analysis. subspace_inference() function from SubspaceInference.jl package used for this analysis. The loss function is modified to accept model as input for subspace inference, because new model parameters will be generated during sampling for the inference. The loss function will be:","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"L1(m, x, y) = Flux.Losses.mse(m(x), y)","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"This example considers subspace size of 3 and the pretrained model is updated for 10 epochs for deviation matrix generation. A new column will be added to deviation matrix during every batch training by setting moment update frequency, c as 1. During inference 100 subspace samples generated by setting itr variable. The subspace inference is generated using RWMH sampling algorithm  with proposal standard deviation of 0.1 as below:","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"M = 3\nT = 10\nc= 1\nσ_z = 0.1\nitr = 100\n #cost function\nall_chain, lp, W_swa = subspace_inference(m, L1, data, opt,   σ_z = 0.1,  itr =itr, T=T, c=1, M=M, print_freq=T, alg =:rwmh);","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The output of subspace_inference() having three variables. chn contains the different DNN weight samples generated from subspace samples. lp is the log probability of each sampling. This helps for subspace inference diagnostics. If the values of lp vector is constant, this means that the proposal by the samplers would be rejected.  If this scenario occurs, we have to try with different subspace size and different proposal distributions. The lp values will look like:","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The effect of weight uncertainty is plotted by predicting output using modified DNN model with weight samples. The model is restructured using re function generated as:","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"θ, re = Flux.destructure(m);","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"For every DNN parameter samples, model is restructured and predicted the output for plotting. To plot, new input z is generated between  -10 to 10. This value is converted to features named inp and it will be fed to restructured DNN model from every parameter samples for the prediction. The predicted output is stored to a array named trajectories. After iteration, it is assigned to a dictionary variable as mentioned above.","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"z = collect(range(-10.0, 10.0,length = 100))\ninp = features(z')\ntrajectories = Array{Float64}(undef,100,itr)\nfor i in 1:itr\n  m1 = re(all_chain[i])\n  out = m1(inp)\n  trajectories[:, i] = out'\nend\nall_trajectories = Dict()\nall_trajectories[\"1\"] = trajectories;","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The effect of DNN parameter uncertainties in output prediction using subspace inference is plotted using following line.","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"SubspaceInference.plot_predictive(data_ld, all_trajectories, title=[\"Plot\"], z)","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"and plotted as in below figure.","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"(Image: Subspace Inference)","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The light blue shaded area represents the effect of DNN parameter uncertainty in output prediction. The mean of uncertainty prediction is plotted in blue color and the red dots corresponds to data points. It is clear from the above figure that the uncertainty is higher in non data areas.","category":"page"},{"location":"nn_example/#Effect-of-different-subspace-sizes-in-uncertainty-analysis","page":"NN Example","title":"Effect of different subspace sizes in uncertainty analysis","text":"","category":"section"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"This experiment analysis the effect of subspace size in uncertainty analysis. We considered subspace size M as 3, 5, 10 and 20. The uncertainty is generated using the following code:","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"\n\nM = [3, 5, 10, 20] #Rank of PCA or Maximum columns in deviation matrix\nT = 5 #Steps\nitr = 100\nall_trajectories = Dict()\nz = collect(range(-10.0, 10.0,length = 100))\ninp = features(z')\nfor mi in 1:4\n    i = 1;\n    @load \"model_weights_$(i).bson\" ps;\n    Flux.loadparams!(m, ps);\n    all_chain, lp, W_swa = subspace_inference(m, L, data, opt,\n  σ_z = 1.0,  itr =itr, T=T, c=1, M=M[mi], print_freq=T, alg =:rwmh);    \n    \n    trajectories = Array{Float64}(undef,100,itr)\n    for i in 1:itr\n        m1 = re(all_chain[i])\n        out = m1(inp)\n        trajectories[:, i] = out'\n    end\n    all_trajectories[\"$(mi)\"] = trajectories;\nend\ntitle = [\"subspace Size: 3\",\"subspace Size: 5\",\"subspace Size: 10\",\"subspace Size: 20\"]\n\nSubspaceInference.plot_predictive(data_ld, all_trajectories, z, title=title)","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"In this code M is considered as an array with subspace sizes. Above figure illustrates the effect DNN parameter uncertainty that generated with different subspace sizes:","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"(Image: Effect of different subspace sizes)","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"Above figure depicts that the uncertainty range is increases with higher subspace sizes.","category":"page"},{"location":"nn_example/#Effect-of-comparison-of-different-proposal-deviations","page":"NN Example","title":"Effect of comparison of different proposal deviations","text":"","category":"section"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"This focuses on the uncertainty outcomes due to different proposal standard deviations, σ_z. This simulation considers proposal deviations of 0.1 and 1.0 and following code is used:","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"M = 10 #Rank of PCA or Maximum columns in deviation matrix\nT = 5 #Steps\nitr = 100\nall_trajectories = Dict()\nz = collect(range(-10.0, 10.0,length = 100))\ninp = features(z')\nσ_z = [0.1, 1.0]\nfor mi in 1:2\n    i = 1;\n    @load \"model_weights_$(i).bson\" ps;\n    Flux.loadparams!(m, ps);\n    all_chain, lp, W_swa = subspace_inference(m, L, data, opt,\n  σ_z = σ_z[mi],  itr =itr, T=T, c=1, M=M, print_freq=T, alg =:rwmh);    \n    \n    trajectories = Array{Float64}(undef,100,itr)\n    for i in 1:itr\n        m1 = re(all_chain[i])\n        out = m1(inp)\n        trajectories[:, i] = out'\n    end\n    all_trajectories[\"$(mi)\"] = trajectories;\nend\ntitle = [\"Std: 0.1\", \"Std: 1.0\"]\n\nSubspaceInference.plot_predictive(data_ld, all_trajectories, z, title=title)","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"(Image: Effect of different proposal standard deviations)","category":"page"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"The above figure shows that for σ_z = 1.0 generates larger uncertainty and it is not fitting mean prediction(blue line) with actual data.","category":"page"},{"location":"nn_example/#Autoencoder-based-subspace-inference","page":"NN Example","title":"Autoencoder based subspace inference","text":"","category":"section"},{"location":"nn_example/","page":"NN Example","title":"NN Example","text":"Autoencoders are a special type of aritifical neural networks where the input is the same as the output. In this work, the encoder part of auto encoder is used to generate ","category":"page"},{"location":"#SubspaceInference.jl","page":"SubspaceInference.jl","title":"SubspaceInference.jl","text":"","category":"section"},{"location":"","page":"SubspaceInference.jl","title":"SubspaceInference.jl","text":"CurrentModule = SubspaceInference\nDocTestSetup = quote\n    using SubspaceInference\nend","category":"page"},{"location":"","page":"SubspaceInference.jl","title":"SubspaceInference.jl","text":"The subspace inference method for Deep Neural Networks (DNN) and ordinary differential equations (ODEs) are implemented as a package named SubspaceInference.jl in Julia.","category":"page"},{"location":"#Subspace-Inference-using-PCA-or-Diffusion-Map","page":"SubspaceInference.jl","title":"Subspace Inference using PCA or Diffusion Map","text":"","category":"section"},{"location":"","page":"SubspaceInference.jl","title":"SubspaceInference.jl","text":"subspace_inference(model, cost, data, opt;σ_z = 1.0, σ_m = 1.0, σ_p = 1.0,\n\titr =1000, T=25, c=1, M=20, print_freq=1, alg =:rwmh, backend = :forwarddiff, method = :subspace)","category":"page"},{"location":"#SubspaceInference.subspace_inference-NTuple{4,Any}","page":"SubspaceInference.jl","title":"SubspaceInference.subspace_inference","text":"subspace_inference(model, cost, data, opt;σ_z = 1.0, σ_m = 1.0, σ_p = 1.0,\nitr =1000, T=25, c=1, M=20, print_freq=1, alg =:rwmh, backend = :forwarddiff, method = :subspace)\n\nTo generate the uncertainty in machine learing models using MH Sampler from subspace\n\nInput Arguments\n\nmodel\t\t: Machine learning model. Eg: Chain(Dense(10,2)). Model should be created with Chain in Flux\ncost\t\t: Cost function. Eg: L(x, y) = Flux.Losses.mse(m(x), y)\ndata\t\t: Inputs and outputs. Eg:\tX = rand(10,100); Y = rand(2,100); data = DataLoader(X,Y);\nopt\t\t\t: Optimzer. Eg: opt = ADAM(0.1)\n\nKeyword Arguments\n\ncallback  \t: Callback function during training. Eg: callback() = @show(L(X,Y))\nσ_z   \t\t: Standard deviation of subspace\nσ_m   \t\t: Standard deviation of likelihood model\nσ_p   \t\t: Standard deviation of prior\nitr\t\t\t: Iterations for sampling\nT\t\t\t: Number of steps for subspace calculation. Eg: T= 1\nc\t\t\t: Moment update frequency. Eg: c = 1\nM\t\t\t: Maximum number of columns in deviation matrix. Eg: M= 3\nalg\t\t\t: Sampling Algorithm. Eg: :rwmh \nbackend\t\t: Differentiation backend. Eg: :forwarddiff\nmethod \t\t: Subspace construction method. Eg: :subspace\nprint_freq: Loss printing frequency\n\nOutput\n\nchn\t\t\t: Chain with samples with uncertainty informations\nlp\t\t\t: Log probabilities of all samples\nW_swa\t\t: Mean Weight\n\n\n\n\n\n","category":"method"},{"location":"#Autoencoder-based-Subspace-Inference","page":"SubspaceInference.jl","title":"Autoencoder based Subspace Inference","text":"","category":"section"},{"location":"","page":"SubspaceInference.jl","title":"SubspaceInference.jl","text":"autoencoder_inference(model, cost, data, opt, encoder, decoder;\n\tσ_z = 1.0,\tσ_m = 1.0, σ_p = 1.0,\n\titr =1000, T=25, c=1, M=20, print_freq=1, alg =:hmc, backend = :forwarddiff)","category":"page"},{"location":"#SubspaceInference.autoencoder_inference-NTuple{6,Any}","page":"SubspaceInference.jl","title":"SubspaceInference.autoencoder_inference","text":"autoencoder_inference(model, cost, data, opt, encoder, decoder;\nσ_z = 1.0,\tσ_m = 1.0, σ_p = 1.0,\nitr =1000, T=25, c=1, M=20, print_freq=1, alg =:hmc, backend = :forwarddiff)\n\nTo generate the uncertainty in machine learing or neural ODE models using auto-encoders\n\nInput Arguments\n\nmodel\t\t: Machine learning model. Eg: Chain(Dense(10,2)). Model should be created with Chain in Flux\ncost\t\t: Cost function. Eg: L(x, y) = Flux.Losses.mse(m(x), y)\ndata\t\t: Inputs and outputs. Eg:\tX = rand(10,100); Y = rand(2,100); data = DataLoader(X,Y);\nopt\t\t\t: Optimzer. Eg: opt = ADAM(0.1)\nencoder\t : Encoder to generate subspace from NN or Neural ODE parameters\ndecoder\t : Decoder to generate NN or Neural ODE parameters from subspace\n\nKeyword Arguments\n\ncallback  \t: Callback function during training. Eg: callback() = @show(L(X,Y))\nσ_z   \t\t: Standard deviation of subspace\nσ_m   \t\t: Standard deviation of likelihood model\nσ_p   \t\t: Standard deviation of prior\nitr\t\t\t: Iterations for sampling\nT\t\t\t: Number of steps for subspace calculation. Eg: T= 1\nc\t\t\t: Moment update frequency. Eg: c = 1\nM\t\t\t: Maximum number of columns in deviation matrix. Eg: M= 3\nalg\t\t\t: Sampling Algorithm. Eg: :rwmh \nbackend\t\t: Differentiation backend. Eg: :forwarddiff\nprint_freq: Loss printing frequency\n\nOutput\n\nchn\t\t\t: Chain with samples with uncertainty informations\nlp\t\t\t: Log probabilities of all samples\n\n\n\n\n\n","category":"method"},{"location":"inference/","page":"Inference","title":"Inference","text":"CurrentModule = SubspaceInference\nDocTestSetup = quote\n    using SubspaceInference\nend","category":"page"},{"location":"inference/#Inference","page":"Inference","title":"Inference","text":"","category":"section"},{"location":"inference/#AdvancedHMC/AdvancedMH/AdvancedNUTS-based-inference","page":"Inference","title":"AdvancedHMC/AdvancedMH/AdvancedNUTS based inference","text":"","category":"section"},{"location":"inference/","page":"Inference","title":"Inference","text":"sub_inference(in_model, data, W_swa, P; σ_z = 1.0, σ_m = 1.0, σ_p = 1.0, itr=100, \n    M = 3, alg = :rwmh,\tbackend = :forwarddiff)","category":"page"},{"location":"inference/#SubspaceInference.sub_inference-NTuple{4,Any}","page":"Inference","title":"SubspaceInference.sub_inference","text":"sub_inference(in_model, data, W_swa, P; σ_z = 1.0, σ_m = 1.0, σ_p = 1.0, itr=100, \nM = 3, alg = :rwmh,\tbackend = :forwarddiff)\n\nTo generate the uncertainty in machine learing models using MH Sampler from subspace\n\nInput Arguments\n\nin_model\t: Machine learning model. Eg: Chain(Dense(10,2)). Model should be created with Chain in Flux\ndata\t\t: Inputs and outputs. Eg:\tX = rand(10,100); Y = rand(2,100); data = DataLoader(X,Y);\nW_swa\t\t: Mean Weight\nP \t\t\t: Projection Matrix\n\nKeyword Arguments\n\nσ_z   \t\t: Standard deviation of subspace\nσ_m   \t\t: Standard deviation of likelihood model\nσ_p   \t\t: Standard deviation of prior\nitr\t\t\t: Iterations for sampling\nM\t\t\t: Maximum number of columns in deviation matrix. Eg: M= 3\nalg\t\t\t: Sampling Algorithm. Eg: :rwmh \nbackend\t\t: Differentiation backend. Eg: :forwarddiff\n\nOutput\n\nchn\t\t\t: Chain with samples with uncertainty informations\nlp\t\t\t: Log probabilities of all samples\n\n\n\n\n\n","category":"method"},{"location":"inference/#Autoencoder-based-inference","page":"Inference","title":"Autoencoder based inference","text":"","category":"section"},{"location":"inference/","page":"Inference","title":"Inference","text":"auto_inference(m, data, decoder, W_swa; σ_z = 1.0,\n\tσ_m = 1.0, σ_p = 1.0, itr=100, M = 3, alg = :hmc,\n\tbackend = :forwarddiff)","category":"page"},{"location":"inference/#SubspaceInference.auto_inference-NTuple{4,Any}","page":"Inference","title":"SubspaceInference.auto_inference","text":"auto_inference(m, data, decoder, W_swa; σ_z = 1.0,\nσ_m = 1.0, σ_p = 1.0, itr=100, M = 3, alg = :hmc,\nbackend = :forwarddiff)\n\nTo generate the uncertainty in machine learing or neural ODE models using auto-encoders\n\nInput Arguments\n\nm\t\t\t: Machine learning model. Eg: Chain(Dense(10,2)). Model should be created with Chain in Flux\ndata\t\t: Inputs and outputs. Eg:\tX = rand(10,100); Y = rand(2,100); data = DataLoader(X,Y);\ndecoder\t \t: Decoder to generate NN or Neural ODE parameters from subspace\nW_swa\t\t: Mean Weight\n\nKeyword Arguments\n\nσ_z   \t\t: Standard deviation of subspace\nσ_m   \t\t: Standard deviation of likelihood model\nσ_p   \t\t: Standard deviation of prior\nitr\t\t\t: Iterations for sampling\nM\t\t\t: Maximum number of columns in deviation matrix. Eg: M= 3\nalg\t\t\t: Sampling Algorithm. Eg: :rwmh \nbackend\t\t: Differentiation backend. Eg: :forwarddiff\n\nOutput\n\nchn\t\t\t: Chain with samples with uncertainty informations\nlp\t\t\t: Log probabilities of all samples\n\n\n\n\n\n","category":"method"},{"location":"inference/#Turing-based-inference","page":"Inference","title":"Turing based inference","text":"","category":"section"},{"location":"inference/","page":"Inference","title":"Inference","text":"turing_inference(m, data, W_swa, P; σ_z = 1.0,\n\tσ_m = 1.0, σ_p = 1.0, itr=100, M = 3, alg = :turing_mh,\n\tbackend = :forwarddiff)","category":"page"},{"location":"inference/#SubspaceInference.turing_inference-NTuple{4,Any}","page":"Inference","title":"SubspaceInference.turing_inference","text":"turing_inference(m, data, W_swa, P; σ_z = 1.0,\nσ_m = 1.0, σ_p = 1.0, itr=100, M = 3, alg = :turing_mh,\nbackend = :forwarddiff)\n\nTo generate the uncertainty in machine learing or neural ODE models using auto-encoders\n\nInput Arguments\n\nm\t\t\t: Machine learning model. Eg: Chain(Dense(10,2)). Model should be created with Chain in Flux\ndata\t\t: Inputs and outputs. Eg:\tX = rand(10,100); Y = rand(2,100); data = DataLoader(X,Y);\nW_swa\t\t: Mean Weight\nP \t\t\t: Projection Matrix\n\nKeyword Arguments\n\nσ_z   \t\t: Standard deviation of subspace\nσ_m   \t\t: Standard deviation of likelihood model\nσ_p   \t\t: Standard deviation of prior\nitr\t\t\t: Iterations for sampling\nM\t\t\t: Maximum number of columns in deviation matrix. Eg: M= 3\nalg\t\t\t: Sampling Algorithm. Eg: :turing_mh \nbackend\t\t: Differentiation backend. Eg: :forwarddiff\n\nOutput\n\nchn\t\t\t: Chain with samples with uncertainty informations\nlp\t\t\t: Log probabilities of all samples\n\n\n\n\n\n","category":"method"}]
}
